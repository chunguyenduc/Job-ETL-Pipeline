version: '3'
x-environment:
    &airflow_environment
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
    - AIRFLOW__CORE__LOAD_EXAMPLES=True
    - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://airflow:airflow@postgres:5432/airflow
    - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
    - AIRFLOW__WEBSERVER__RBAC=False
    - AIRFLOW__WEBSERVER__COOKIE_SAMESITE=None


x-airflow-image: &airflow_image apache/airflow:2.3.2-python3.8
x-superset-image: &superset-image apache/superset:latest
x-superset-user: &superset-user root
x-superset-depends-on:
    &superset-depends-on
    - postgres
    - redis
x-superset-volumes:
    # /app/pythonpath_docker will be appended to the PYTHONPATH in the final container
    &superset-volumes
    - ./superset/docker:/app/docker
    - superset_home:/app/superset_home
    - ./superset/docker/pythonpath_dev:/app/pythonpath

services:
    redis:
        image: redis:latest
    postgres:
        image: postgres:latest
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        logging:
            options:
                max-size: 10m
                max-file: "3"
        ports:
            - "5432:5432"
    initdb_adduser:
        build:
            context: ./airflow
            args:
                AIRFLOW_BASE_IMAGE: *airflow_image
        depends_on:
            - postgres
        environment: *airflow_environment
        container_name: airflow_init
        entrypoint: /bin/bash
        command: -c 'airflow db upgrade && airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org && airflow sync-perm'

    webserver:
        build:
            context: ./airflow
            args:
                AIRFLOW_BASE_IMAGE: *airflow_image
        container_name: airflow_webserver
        depends_on:
            - postgres
        volumes:
            - ./airflow/logs:/opt/airflow/logs
            - ./airflow/dags:/opt/airflow/dags
            - ./airflow/plugins:/opt/airflow/plugins
        ports:
            - "8080:8080"
        restart: on-failure
        environment: *airflow_environment
        command: webserver

    scheduler:
        build:
            context: ./airflow
            args:
                AIRFLOW_BASE_IMAGE: *airflow_image
        container_name: airflow_scheduler
        depends_on:
            - postgres
        restart: on-failure
        volumes:
            - ./airflow/dags:/opt/airflow/dags
            - ./airflow/logs:/opt/airflow/logs
        environment: *airflow_environment
        command: scheduler

    namenode:
        image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
        container_name: namenode
        ports:
            - 50070:50070
        volumes:
            - hadoop_namenode:/hadoop/dfs/name
        environment:
            - CLUSTER_NAME=test
        env_file:
            - .env/hadoop.env

    datanode:
        image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
        container_name: datanode
        ports:
            - 50075:50075
        volumes:
            - hadoop_datanode:/hadoop/dfs/data
        environment:
            SERVICE_PRECONDITION: "namenode:50070"
        env_file:
            - .env/hadoop.env

    resourcemanager:
        image: bde2020/hadoop-resourcemanager:2.0.0-hadoop2.7.4-java8
        container_name: resourcemanager
        environment:
            SERVICE_PRECONDITION: "namenode:50070 datanode:50075"
        env_file:
            - .env/hadoop.env

    nodemanager1:
        image: bde2020/hadoop-nodemanager:2.0.0-hadoop2.7.4-java8
        container_name: nodemanager1
        environment:
            SERVICE_PRECONDITION: "namenode:50070 datanode:50075 resourcemanager:8088"
        env_file:
            - .env/hadoop.env

    historyserver:
        image: bde2020/hadoop-historyserver:2.0.0-hadoop2.7.4-java8
        container_name: historyserver
        environment:
            SERVICE_PRECONDITION: "namenode:50070 datanode:50075 resourcemanager:8088"
        volumes:
            - hadoop_historyserver:/hadoop/yarn/timeline
        env_file:
            - .env/hadoop.env
    spark:
        build:
            context: spark
        user: root
        container_name: spark
        environment:
            - SPARK_MODE=master
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
            - SPARK_EXECUTOR_MEMORY=4G

        volumes:
            - ./airflow/dags/transform:/opt/airflow/dags/transform
        ports:
            - "8181:8080"
            - "7077:7077"
    spark-worker:
        build:
            context: spark
        user: root
        depends_on:
            - spark
        container_name: spark_worker
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=8G
            - SPARK_EXECUTOR_MEMORY=4G
            - SPARK_WORKER_CORES=4
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./airflow/dags/transform:/opt/airflow/dags/transform
    hive-server:
        image: bde2020/hive:2.3.2-postgresql-metastore
        container_name: hive_server
        env_file:
            - .env/hadoop-hive.env
        environment:
            HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
            SERVICE_PRECONDITION: "hive-metastore:9083"
        ports:
            - "10000:10000"
        restart: on-failure
    hive-metastore:
        image: bde2020/hive:2.3.2-postgresql-metastore
        container_name: hive_metastore
        env_file:
            - .env/hadoop-hive.env
        command: /opt/hive/bin/hive --service metastore
        environment:
            SERVICE_PRECONDITION: "namenode:50070 datanode:50075 hive-metastore-postgresql:5432"
        ports:
            - "9083:9083"
    hive-metastore-postgresql:
        image: bde2020/hive-metastore-postgresql:2.3.0
        container_name: hive_metastore_postgresql

    superset:
        image: *superset-image
        container_name: superset
        command: [ "/app/docker/docker-bootstrap.sh", "app" ]
        restart: unless-stopped
        ports:
            - 8088:8088
        user: *superset-user
        depends_on: *superset-depends-on
        volumes: *superset-volumes

    superset-init:
        image: *superset-image
        container_name: superset_init
        command: [ "/app/docker/docker-init.sh" ]
        depends_on: *superset-depends-on
        user: "root"
        volumes: *superset-volumes
        environment:
            SUPERSET_LOAD_EXAMPLES: "yes"
            SUPERSET_CONFIG_PATH: "/app/pythonpath/superset_config.py"
    prometheus:
        image: prom/prometheus:latest
        container_name: monitoring_prometheus
        restart: unless-stopped
        volumes:
        - ./data/prometheus/config:/etc/prometheus/
        - ./data/prometheus/data:/prometheus
        command:
        - '--config.file=/etc/prometheus/prometheus.yml'
        expose:
            - 9090
        ports:
            - 9090:9090
        links:
        - cadvisor:cadvisor
        - node-exporter:node-exporter

    node-exporter:
        image: prom/node-exporter:latest
        container_name: monitoring_node_exporter
        restart: unless-stopped
        expose:
            - 9100

    cadvisor:
        image: gcr.io/cadvisor/cadvisor:latest
        container_name: monitoring_cadvisor
        restart: unless-stopped
        volumes:
            - /:/rootfs:ro
            - /var/run:/var/run:rw
            - /sys:/sys:ro
            - /var/lib/docker/:/var/lib/docker:ro
            - /var/run/docker.sock:/var/run/docker.sock:rw
        ports:
            - 8083:8080
  
    grafana:
        image: grafana/grafana:latest
        container_name: monitoring_grafana
        restart: unless-stopped
        links:
            - prometheus:prometheus
        # volumes:
        #     - ./data/grafana:/var/lib/grafana
        environment:
            - GF_SECURITY_ADMIN_USER=admin
            - GF_SECURITY_ADMIN_PASSWORD=admin
            - GF_USERS_ALLOW_SIGN_UP=false
        ports:
            - 3000:3000

volumes:
    hadoop_namenode:
    hadoop_datanode:
    hadoop_historyserver:
    superset_home:
        external: false
