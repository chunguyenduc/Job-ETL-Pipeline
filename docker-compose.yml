version: '3'
x-environment:
    &airflow_environment
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
    - AIRFLOW__CORE__LOAD_EXAMPLES=True
    - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://airflow:airflow@postgres:5432/airflow
    - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
    - AIRFLOW__WEBSERVER__RBAC=False

x-airflow-image: &airflow_image apache/airflow:2.3.2-python3.8

services:
    redis:
        image: redis:latest
    postgres:
        image: postgres:latest
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        logging:
            options:
                max-size: 10m
                max-file: "3"
        ports:
            - "5432:5432"
    initdb_adduser:
        build:
            context: airflow
            args:
                AIRFLOW_BASE_IMAGE: *airflow_image
        depends_on:
            - postgres
        environment: *airflow_environment
        entrypoint: /bin/bash
        command: -c 'airflow db upgrade && yes | airflow db reset && airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org'

    webserver:
        build:
            context: airflow
            args:
                AIRFLOW_BASE_IMAGE: *airflow_image
        # restart: always
        depends_on:
            - postgres
        volumes:
            - ./airflow/logs:/opt/airflow/logs
            - ./airflow/dags:/opt/airflow/dags
            - ./airflow/plugins:/opt/airflow/plugins
        ports:
            - "8080:8080"
        environment: *airflow_environment
        command: webserver

    scheduler:
        build:
            context: airflow
            args:
                AIRFLOW_BASE_IMAGE: *airflow_image
        # restart: always
        depends_on:
            - postgres
        volumes:
            - ./airflow/dags:/opt/airflow/dags
            - ./airflow/logs:/opt/airflow/logs
        environment: *airflow_environment
        command: scheduler

    namenode:
        image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
        container_name: namenode
        restart: always
        ports:
            - 50070:50070
        volumes:
            - hadoop_namenode:/hadoop/dfs/name
        environment:
            - CLUSTER_NAME=test
        env_file:
            - ./hadoop.env

    datanode:
        image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
        container_name: datanode
        restart: always
        ports:
            - 50075:50075
        volumes:
            - hadoop_datanode:/hadoop/dfs/data
        environment:
            SERVICE_PRECONDITION: "namenode:50070"
        env_file:
            - ./hadoop.env

    resourcemanager:
        image: bde2020/hadoop-resourcemanager:2.0.0-hadoop2.7.4-java8
        restart: always
        environment:
            SERVICE_PRECONDITION: "namenode:50070 datanode:50075"
        env_file:
            - ./hadoop.env

    nodemanager1:
        image: bde2020/hadoop-nodemanager:2.0.0-hadoop2.7.4-java8
        restart: always
        environment:
            SERVICE_PRECONDITION: "namenode:50070 datanode:50075 resourcemanager:8088"
        env_file:
            - ./hadoop.env

    historyserver:
        image: bde2020/hadoop-historyserver:2.0.0-hadoop2.7.4-java8
        restart: always
        environment:
            SERVICE_PRECONDITION: "namenode:50070 datanode:50075 resourcemanager:8088"
        volumes:
            - hadoop_historyserver:/hadoop/yarn/timeline
        env_file:
            - ./hadoop.env
    spark:
        image: bitnami/spark:3.1.2
        user: root # Run container as root container: https://docs.bitnami.com/tutorials/work-with-non-root-containers/
        container_name: spark
        environment:
            - SPARK_MODE=master
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./airflow/dags/transform:/usr/local/airflow/dags/transform
        ports:
            - "7077:7077"

    hive-server:
        image: bde2020/hive:2.3.2-postgresql-metastore
        env_file:
            - ./hadoop-hive.env
        environment:
            HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
            SERVICE_PRECONDITION: "hive-metastore:9083"
        ports:
            - "10000:10000"
    hive-metastore:
        image: bde2020/hive:2.3.2-postgresql-metastore
        env_file:
            - ./hadoop-hive.env
        command: /opt/hive/bin/hive --service metastore
        environment:
            SERVICE_PRECONDITION: "namenode:50070 datanode:50075 hive-metastore-postgresql:5432"
        ports:
            - "9083:9083"
    hive-metastore-postgresql:
        image: bde2020/hive-metastore-postgresql:2.3.0
    presto-coordinator:
        image: shawnzhu/prestodb:0.181
        ports:
            - "8081:8081"
             # airflow-webserver:
    #     build:
    #         context: ./airflow
    #         dockerfile: Dockerfile
    #     # restart: always
    #     depends_on:
    #         - postgres
    #     environment:
    #         - LOAD_EX=y
    #         - EXECUTOR=Local
    #     logging:
    #         options:
    #             max-size: 10m
    #             max-file: "3"
    #     volumes:
    #         - ./airflow/dags:/usr/local/airflow/dags
    #         - ./airflow/plugins:/usr/local/airflow/plugins
    #     ports:
    #         - "8080:8080"
    #     command: webserver
    #     healthcheck:
    #         test:
    #             [
    #                 "CMD-SHELL",
    #                 "[ -f /usr/local/airflow/airflow-webserver.pid ]"
    #             ]
    #         interval: 30s
    #         timeout: 30s
    #         retries: 3
volumes:
    hadoop_namenode:
    hadoop_datanode:
    hadoop_historyserver:
