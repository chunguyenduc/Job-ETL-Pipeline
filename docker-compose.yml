version: '3'
services:
    redis:
        image: redis:latest
    postgres:
        image: postgres:latest
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        logging:
            options:
                max-size: 10m
                max-file: "3"

    airflow-webserver:
        build:
            context: ./airflow
            dockerfile: Dockerfile
        # restart: always
        depends_on:
            - postgres
        environment:
            - LOAD_EX=y
            - EXECUTOR=Local
        logging:
            options:
                max-size: 10m
                max-file: "3"
        volumes:
            - ./airflow/dags:/usr/local/airflow/dags
            - ./airflow/plugins:/usr/local/airflow/plugins
        ports:
            - "8080:8080"
        command: webserver
        healthcheck:
            test:
                [
                    "CMD-SHELL",
                    "[ -f /usr/local/airflow/airflow-webserver.pid ]"
                ]
            interval: 30s
            timeout: 30s
            retries: 3
    namenode:
        image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
        container_name: namenode
        restart: always
        ports:
            - 50070:50070
        volumes:
            - hadoop_namenode:/hadoop/dfs/name
        environment:
            - CLUSTER_NAME=test
        env_file:
            - ./hadoop.env

    datanode:
        image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
        container_name: datanode
        restart: always
        ports:
            - 50075:50075
        volumes:
            - hadoop_datanode:/hadoop/dfs/data
        environment:
            SERVICE_PRECONDITION: "namenode:50070"
        env_file:
            - ./hadoop.env

    resourcemanager:
        image: bde2020/hadoop-resourcemanager:2.0.0-hadoop2.7.4-java8
        restart: always
        environment:
            SERVICE_PRECONDITION: "namenode:50070 datanode:50075"
        env_file:
            - ./hadoop.env

    nodemanager1:
        image: bde2020/hadoop-nodemanager:2.0.0-hadoop2.7.4-java8
        restart: always
        environment:
            SERVICE_PRECONDITION: "namenode:50070 datanode:50075 resourcemanager:8088"
        env_file:
            - ./hadoop.env

    historyserver:
        image: bde2020/hadoop-historyserver:2.0.0-hadoop2.7.4-java8
        restart: always
        environment:
            SERVICE_PRECONDITION: "namenode:50070 datanode:50075 resourcemanager:8088"
        volumes:
            - hadoop_historyserver:/hadoop/yarn/timeline
        env_file:
            - ./hadoop.env
    spark:
        image: bitnami/spark:3.1.2
        user: root # Run container as root container: https://docs.bitnami.com/tutorials/work-with-non-root-containers/
        container_name: spark
        environment:
            - SPARK_MODE=master
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./airflow/dags/transform:/usr/local/airflow/dags/transform
        ports:
            - "7077:7077"
volumes:
    hadoop_namenode:
    hadoop_datanode:
    hadoop_historyserver:
